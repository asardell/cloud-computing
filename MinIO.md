#  MinIO

<p align="center">
  <img src="https://blog.min.io/content/images/2019/05/MINIO_wordmark.png" alt="Source de l'image" width="600"/>
</p>

- [MinIO](#minio)
- [Introduction √† MinIO](#introduction-√†-minio)
  - [Pourquoi MinIO en entreprise et pour le cloud souverain ?](#pourquoi-minio-en-entreprise-et-pour-le-cloud-souverain-)
    - [Concurrents principaux](#concurrents-principaux)
  - [Concepts cl√©s](#concepts-cl√©s)
    - [Buckets](#buckets)
    - [Stockage de fichiers vs SGBD](#stockage-de-fichiers-vs-sgbd)
    - [Data Lake et couches Medaillon](#data-lake-et-couches-medaillon)
  - [Formats de fichiers](#formats-de-fichiers)
    - [Pourquoi le partitionnement est essentiel en Data Lake ?](#pourquoi-le-partitionnement-est-essentiel-en-data-lake-)
    - [Cardinalit√© : notion cl√©](#cardinalit√©--notion-cl√©)
    - [Bonne pratique](#bonne-pratique)
    - [Mauvais exemple](#mauvais-exemple)
    - [Pourquoi c‚Äôest strat√©gique en entreprise ?](#pourquoi-cest-strat√©gique-en-entreprise-)
    - [Astuces : Lecture d'un fichier parquet](#astuces--lecture-dun-fichier-parquet)
  - [Apache Iceberg](#apache-iceberg)
    - [Qu‚Äôest-ce qu‚ÄôApache Iceberg ?](#quest-ce-quapache-iceberg-)
    - [Pourquoi ACID est important ?](#pourquoi-acid-est-important-)
    - [Pourquoi Iceberg est int√©ressant](#pourquoi-iceberg-est-int√©ressant)
    - [Cas d‚Äôusage typique](#cas-dusage-typique)
    - [Conclusion](#conclusion)
  - [MinIO et Python](#minio-et-python)
  - [TD : MinIO et donn√©es ADEME](#td--minio-et-donn√©es-ademe)
    - [Arborescence du TP](#arborescence-du-tp)
    - [docker-compose.yml](#docker-composeyml)
      - [1. Service `minio`](#1-service-minio)
      - [2. Service `python`](#2-service-python)
    - [requirements.txt](#requirementstxt)
    - [Dockerfile](#dockerfile)
    - [Script ‚Äì `fetch_ademe_to_minio.py`](#script--fetch_ademe_to_miniopy)
    - [Script ‚Äì `json_to_csv.py`](#script--json_to_csvpy)
    - [Script ‚Äì `csv_to_parquet.py`](#script--csv_to_parquetpy)
    - [Commandes docker](#commandes-docker)
    - [R√©sultat  attendu](#r√©sultat--attendu)
    - [Tester la persistance des donn√©es MinIO avec les volumes Docker](#tester-la-persistance-des-donn√©es-minio-avec-les-volumes-docker)
  - [TD : Gestion des utilisateurs et policies](#td--gestion-des-utilisateurs-et-policies)
    - [Arborescence du projet](#arborescence-du-projet)
    - [ubuntu/Dockerfile](#ubuntudockerfile)
    - [policies/read-policy.json](#policiesread-policyjson)
    - [policies/write-policy.json](#policieswrite-policyjson)
    - [python/test\_users.py](#pythontest_userspy)
    - [Commandes √† ex√©cuter](#commandes-√†-ex√©cuter)
  - [TD : MinIO et Apache Iceberg](#td--minio-et-apache-iceberg)
    - [Arborescence du projet](#arborescence-du-projet-1)
    - [Configuration Spark](#configuration-spark)
    - [Commandes Docker expliqu√©es](#commandes-docker-expliqu√©es)


# Introduction √† MinIO

**Objectif :** D√©couvrir MinIO, comprendre ses concepts cl√©s et son utilit√© pour le stockage d‚Äôobjets dans le cloud, en particulier pour des projets p√©dagogiques et le cloud souverain.

MinIO est un **syst√®me de stockage d‚Äôobjets** compatible avec l‚ÄôAPI S3 d‚ÄôAWS.  
Il permet de stocker et r√©cup√©rer des fichiers (objets) de mani√®re simple et rapide, √† petite ou grande √©chelle.

## Pourquoi MinIO en entreprise et pour le cloud souverain ?

- **Compatibilit√© S3** : fonctionne comme AWS S3, donc facile √† int√©grer dans des projets Python, Spark, ou Hadoop via **Boto3** ou d‚Äôautres clients S3.  
- **Cloud souverain** : solution open-source adapt√©e aux besoins de souverainet√© num√©rique. Les donn√©es restent sur l‚Äôinfrastructure locale ou nationale.  
- **P√©dagogique et gratuite** : id√©al pour apprendre le stockage d‚Äôobjets, manipuler des buckets et tester des pipelines de donn√©es sans co√ªt cloud.  
- **Scalabilit√©** : peut g√©rer de petits projets comme des volumes massifs de donn√©es.  

### Concurrents principaux

| Solution | Particularit√© |
|----------|---------------|
| AWS S3 | Stockage d‚Äôobjets cloud leader, robuste, riche en fonctionnalit√©s |
| Azure Storage Account | Stockage cloud Microsoft, int√©gration avec Azure Data Factory |
| Google Cloud Storage | Stockage cloud Google, compatible S3 via certains outils |
| MinIO | Open-source, l√©ger, compatible S3, d√©ployable localement ou en cloud souverain |


## Concepts cl√©s

### Buckets

- Un **bucket** est un conteneur logique pour stocker des objets (fichiers).  
- Chaque objet a une **cl√©** unique dans le bucket.  
- On peut avoir plusieurs buckets pour organiser les donn√©es par projet, type ou environnement.

<p align="center">
  <img src="https://miro.medium.com/v2/resize:fit:1200/1*edAFf8onuLXdRGGn2Uu5jQ.png" alt="Source de l'image" width="600"/>
</p>


### Stockage de fichiers vs SGBD

- MinIO stocke des fichiers, contrairement aux SGBD qui stockent des **donn√©es structur√©es** dans des tables.  
- Id√©al pour les **fichiers volumineux**, les logs, les images, vid√©os, JSON, CSV, Parquet‚Ä¶  
- Les fichiers peuvent √™tre lus par des outils de traitement de donn√©es, ETL, Spark, ou Pandas.

<p align="center">
  <img src="https://preprod.leviia.com/wp-content/uploads/2023/07/stockage-objet-bucket-s3.png" alt="Source de l'image" width="600"/>
</p>


### Data Lake et couches Medaillon

- MinIO est souvent utilis√© pour constituer un **Data Lake**, c‚Äôest-√†-dire un entrep√¥t de donn√©es centralis√© qui accepte tous types de formats.  
- Exemple de couches Medaillon :
  - **Bronze** : donn√©es brutes, telles qu‚Äôelles sont collect√©es
  - **Silver** : donn√©es nettoy√©es, enrichies, pr√™tes pour analyse
  - **Gold** : donn√©es consolid√©es et optimis√©es pour reporting ou machine learning

<p align="center">
  <img src="https://www.databricks.com/sites/default/files/inline-images/building-data-pipelines-with-delta-lake-120823.png" alt="Source de l'image" width="600"/>
</p>

## Formats de fichiers

| Format | Particularit√© | Usage recommand√© |
|--------|---------------|----------------|
| JSON | Semi-structur√©, flexible, mais volumineux | Donn√©es brutes ou API |
| CSV | Structur√©, simple | Transformation et √©change de donn√©es |
| Parquet | Colonnaire, compress√©, partitionnable | Analytics, data lake, Big Data |
  
<p align="center">
  <img src="https://substackcdn.com/image/fetch/$s_!4vTf!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7b4d4edc-20dd-4480-8b33-e11705fcd61f_2000x1428.png" alt="Source de l'image" width="600"/>
</p>

Le **partitionnement** consiste √† **d√©couper un jeu de donn√©es en plusieurs sous-dossiers** en fonction de la valeur d‚Äôune ou plusieurs colonnes.

Exemple :

```
final/parquet/
 ‚îú‚îÄ‚îÄ etiquette_dpe=A/
 ‚îú‚îÄ‚îÄ etiquette_dpe=B/
 ‚îú‚îÄ‚îÄ etiquette_dpe=C/
 ‚îî‚îÄ‚îÄ etiquette_dpe=D/
```

Chaque dossier contient uniquement les lignes correspondant √† la valeur de la partition.

### Pourquoi le partitionnement est essentiel en Data Lake ?

Sans partitionnement :
- le moteur de calcul (Spark, Pandas, DuckDB‚Ä¶) doit lire **tout le dataset**

Avec partitionnement :
- seules les partitions utiles sont lues  --> Gain √©norme en **temps de calcul** et en **co√ªt**


### Cardinalit√© : notion cl√©

La **cardinalit√©** correspond au **nombre de valeurs distinctes** d‚Äôune colonne.

Types de cardinalit√©

| Type | Exemple | Caract√©ristique |
|------|---------|-----------------|
| Faible cardinalit√© ‚úÖ | `etiquette_dpe` (A, B, C, D, E) | Tr√®s peu de valeurs |
| Moyenne | `code_departement` (01 ‚Üí 95) | Plusieurs dizaines |
| Haute cardinalit√© ‚ùå | `id_unique`, `numero_dpe` | Quasi unique par ligne |

### Bonne pratique

On **partitionne de pr√©f√©rence sur des colonnes √† faible cardinalit√©**, comme :

- cat√©gories (`etiquette_dpe`)
- pays
- d√©partement
- type de produit
- date

### Mauvais exemple

Partitionner sur :

- un identifiant unique
- un num√©ro de facture
- une cl√© primaire

Cela cr√©e des **milliers de dossiers vides ou √† une seule ligne**, ce qui d√©grade les performances.


### Pourquoi c‚Äôest strat√©gique en entreprise ?

Le partitionnement est utilis√© massivement dans :

- Data Lakes AWS S3
- Azure Data Lake
- Google Cloud Storage
- Spark
- Hive
- Databricks

Il permet de :

- acc√©l√©rer les requ√™tes  
- r√©duire les co√ªts de lecture  
- mieux organiser les donn√©es  

### Astuces : Lecture d'un fichier parquet

Les fichiers **Parquet** sont **colonnaires et optimis√©s pour le traitement Big Data**, mais ils ne sont pas facilement lisibles dans un simple ou √©diteur de texte classique.  

Pour visualiser ou explorer un fichier Parquet localement, on peut utiliser des outils **open source** comme [Parquet Viewer](https://github.com/mukunku/ParquetViewer). Une application graphique pour ouvrir et explorer les fichiers Parquet.

:bulb: [T√©l√©charger le fichier `ParquetViewer.exe`](https://github.com/mukunku/ParquetViewer/releases/download/v3.5.0.2/ParquetViewer.exe)


<p align="center">
  <img src="https://github.com/mukunku/ParquetViewer/raw/main/wiki_images/main_screenshot5.png" alt="Source de l'image" width="600"/>
</p>


## Apache Iceberg

<p align="center">
  <img src="https://cdn.prod.website-files.com/60f955236a773f743298d63b/64ba8c146aed466249378cb1_AnyConv.com__image6.webp" alt="Source de l'image" width="600"/>
</p>


### Qu‚Äôest-ce qu‚ÄôApache Iceberg ?

Apache Iceberg est un **format de table open source pour les Data Lakes** qui apporte :

- Une **gestion ACID compl√®te** (Atomicit√©, Coh√©rence, Isolation, Durabilit√©) sur des fichiers stock√©s dans des syst√®mes distribu√©s (S3, ADLS, GCS‚Ä¶).  
- La possibilit√© de **travailler sur des tables immuables avec des snapshots**, ce qui rend les op√©rations comme **update, delete, merge** possibles sur un Data Lake.  
- Une compatibilit√© avec des moteurs de calcul comme **Spark, Flink, Trino, Hive**.

### Pourquoi ACID est important ?

ACID est un acronyme pour :

| Lettre | Signification | Pourquoi c‚Äôest utile |
|--------|---------------|--------------------|
| A | Atomicit√© | Chaque transaction est **tout ou rien** : aucune donn√©e partiellement √©crite |
| C | Coh√©rence | Les donn√©es restent **coh√©rentes** apr√®s chaque transaction |
| I | Isolation | Plusieurs jobs ou utilisateurs peuvent √©crire/lire simultan√©ment **sans conflit** |
| D | Durabilit√© | Une fois une transaction valid√©e, elle est **persistante**, m√™me en cas de panne |

Sans ACID, les Data Lakes peuvent devenir **instables** lorsqu‚Äôon fait des mises √† jour ou des suppressions, ce qui peut provoquer des **donn√©es corrompues ou incoh√©rentes**.

### Pourquoi Iceberg est int√©ressant

1. **Gestion efficace des fichiers**  
   - Iceberg maintient un **catalogue de m√©tadonn√©es**.  
   - Permet d‚Äô√©viter de scanner **des milliers de fichiers** √† chaque requ√™te.  
   - Compatible avec **partitionnement et clustering**.

2. **Support ACID sur un Data Lake**  
   - Les tables Iceberg permettent les **UPDATE, DELETE, MERGE** alors que des formats classiques comme Parquet ne le permettent pas nativement.  
   - Id√©al pour des pipelines de donn√©es **modifiables**.

3. **Snapshots et time travel**  
   - On peut revenir √† un **ancien √©tat de la table** en quelques lignes de code.  
   - Tr√®s utile pour **audits, debugging, et pipelines reproductibles**.

4. **Performance et scalabilit√©**  
   - Partitionnement intelligent et indexation des fichiers.  
   - Requ√™tes plus rapides sur de grands datasets m√™me dans le Cloud.

### Cas d‚Äôusage typique

- **Data Lake d‚Äôentreprise** : stockage de donn√©es transactionnelles et analytiques avec possibilit√© de modifications.  
- **Tables √©v√©nementielles** : logs, clics web, transactions financi√®res.  
- **Machine Learning** : r√©entra√Æner des mod√®les avec des versions stables des datasets.


### Conclusion

- Iceberg apporte **robustesse et flexibilit√©** aux Data Lakes.  
- Avec ACID et snapshots, on peut faire du **traitement batch ou streaming** sur des donn√©es fiables.  
- Il combine **les avantages des Data Warehouses (fiabilit√©, coh√©rence)** et des **Data Lakes (scalabilit√©, co√ªt r√©duit)**.


## MinIO et Python

L‚Äô**API S3** (Simple Storage Service) est un standard pour le stockage d‚Äôobjets dans le cloud.  
Elle a √©t√© cr√©√©e par **AWS S3** et est aujourd‚Äôhui reprise par de nombreux services :

- Amazon S3 (AWS)
- Azure Blob Storage (via couches de compatibilit√©)
- Google Cloud Storage
- MinIO

:bulb: Cela signifie qu‚Äôun m√™me code peut fonctionner sur plusieurs fournisseurs de cloud.

- Gr√¢ce √† la **compatibilit√© S3**, on peut utiliser **Boto3**, le client officiel AWS, pour interagir avec MinIO :  
  - Cr√©er des buckets  
  - Uploader et t√©l√©charger des fichiers  
  - Lister ou supprimer des objets  

:bulb: Cela permet  de se familiariser avec le stockage cloud m√™me sans abonnement AWS ou Azure.

<p align="center">
  <img src="https://miro.medium.com/v2/resize:fit:1200/1*rUUJdOUmInl-lXPq2hO4jA.jpeg" alt="Source de l'image" width="600"/>
</p>

## TD : MinIO et donn√©es ADEME

Ce TP permet de mettre en pratique les concepts de **stockage objet**, **buckets**, **formats de fichiers** et **data lake**, tout en utilisant Python et Boto3 dans un environnement local gratuit.
Nous allons manipuler MinIO pour cr√©er un **pipeline de donn√©es** bas√© sur les donn√©es publiques de l‚ÄôADEME.


Nous utilisons **Docker Compose** pour orchestrer deux services :

- **MinIO** : serveur de stockage d‚Äôobjets (√©quivalent local d‚ÄôAWS S3).
- **Python** : conteneur qui ex√©cutera nos scripts pour :
  - appeler l‚ÄôAPI ADEME
  - sauvegarder les donn√©es dans MinIO
  - transformer les formats (JSON ‚Üí CSV ‚Üí Parquet).

Sch√©ma logique :

```
API ADEME ‚îÄ‚îÄ> Python ‚îÄ‚îÄ> MinIO (Bucket)
```

Le projet vise √† simuler un **Data Lake local avec MinIO**, structur√© en couches :

```
Bucket MinIO
 ‚îú‚îÄ‚îÄ bronze/   (donn√©es brutes JSON)
 ‚îú‚îÄ‚îÄ silver/   (donn√©es nettoy√©es en CSV)
 ‚îî‚îÄ‚îÄ gold/     (donn√©es optimis√©es en Parquet partitionn√©)
```

Lien avec AWS S3 & Boto3

MinIO est **compatible S3**, donc :
- Le code que les √©tudiants apprennent ici peut √™tre r√©utilis√© plus tard sur :
  - AWS S3
  - Azure Blob Storage
  - Google Cloud Storage

C‚Äôest pour cela que l‚Äôon utilise **Boto3** : c‚Äôest une comp√©tence transf√©rable vers le monde professionnel.

### Arborescence du TP

```text
tp-minio-ademe/
‚îú‚îÄ‚îÄ docker-compose.yml
‚îú‚îÄ‚îÄ python/
‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile
‚îÇ   ‚îú‚îÄ‚îÄ requirements.txt
‚îÇ   ‚îú‚îÄ‚îÄ fetch_ademe_to_minio.py # R√©cup√®re les donn√©es ADEME et les stocke dans MinIO
‚îÇ   ‚îú‚îÄ‚îÄ json_to_csv.py          # Transforme les fichiers JSON en CSV
‚îÇ   ‚îî‚îÄ‚îÄ csv_to_parquet.py       # Convertit les CSV en Parquet avec partitionnement
```

### docker-compose.yml

```yaml
services: 
  minio:
    image: minio/minio
    container_name: minio
    ports:
      - "9000:9000"
      - "9001:9001"
    environment:
      MINIO_ROOT_USER: minioadmin
      MINIO_ROOT_PASSWORD: minioadmin
    command: server /data --console-address ":9001"
    volumes:
      - minio_data:/data

  python:
    build:
      context: ./python
    container_name: minio-python
    depends_on:
      - minio
    volumes:
      - ./python:/app
    working_dir: /app
    command: python fetch_ademe_to_minio.py

volumes:
  minio_data:
```

Explication

#### 1. Service `minio`

```yaml
image: minio/minio
```
Nous utilisons l‚Äôimage officielle de MinIO.  
Cela nous permet de simuler un **S3 local**, sans passer par AWS.

```yaml
ports:
  - "9000:9000"
  - "9001:9001"
```
- Port **9000** ‚Üí API S3 (utilis√©e par Boto3)
- Port **9001** ‚Üí Console web MinIO (interface graphique)

üëâ Apr√®s le lancement, tu peux acc√©der √† l‚Äôinterface :  
`http://localhost:9001`

```yaml
environment:
  MINIO_ROOT_USER: minioadmin
  MINIO_ROOT_PASSWORD: minioadmin
```
Identifiants pour se connecter √† la console MinIO.

```yaml
command: server /data --console-address ":9001"
```
- D√©marre le serveur MinIO
- Stocke les fichiers dans `/data`
- Active la console web

```yaml
volumes:
  - minio_data:/data
```
**Point cl√© :**  
Ce volume permet de **persister les fichiers m√™me si le conteneur est supprim√©**.  
C‚Äôest tr√®s proche du fonctionnement d‚Äôun vrai data lake.


#### 2. Service `python`

```yaml
build: .
```
Construit l‚Äôimage √† partir du `Dockerfile`.

```yaml
depends_on:
  - minio
```
Forcer Docker √† d√©marrer MinIO avant le script Python.

```yaml
volumes:
  - ./python:/app
```
Permet de modifier les scripts localement sans reconstruire l‚Äôimage.

```yaml
command: python fetch_ademe_to_minio.py
```
Lance automatiquement le script de r√©cup√©ration des donn√©es ADEME.


### requirements.txt

```text
boto3
requests
pandas
pyarrow
```

R√¥le de chaque biblioth√®que

- **boto3** : SDK AWS compatible avec MinIO ‚Üí permet de parler en **langage S3**
- **requests** : appeler l‚ÄôAPI ADEME
- **pandas** : manipuler les donn√©es sous forme de DataFrame
- **pyarrow** : √©crire des fichiers **Parquet**


### Dockerfile

```dockerfile
# Image Python officielle
FROM python:3.11.8-slim

WORKDIR /app

# Copier requirements
COPY requirements.txt /app/requirements.txt

# Installer d√©pendances
RUN pip install --no-cache-dir -r requirements.txt

# Copier tous les scripts Python
COPY . /app/
```

Explication

```dockerfile
FROM python:3.11.8-slim
```
Image officielle Python, l√©g√®re et optimis√©e.

```dockerfile
WORKDIR /app
```
R√©pertoire de travail par d√©faut dans le conteneur.

```dockerfile
COPY requirements.txt /app/requirements.txt
RUN pip install --no-cache-dir -r requirements.txt
```
Installation des d√©pendances.

```dockerfile
COPY . /app/
```
Copie de tous les scripts Python dans le conteneur.

### Script ‚Äì `fetch_ademe_to_minio.py`

Objectif : r√©cup√©rer les donn√©es depuis l‚ÄôAPI ADEME et les stocker en **JSON brut** dans MinIO.

```python
import requests
import json
import boto3
from pathlib import Path

# ------------------------------------------------------------
# CONFIGURATION API ADEME
# ------------------------------------------------------------
# URL de l‚ÄôAPI publique ADEME (source de donn√©es ouverte)
API_URL = "https://data.ademe.fr/data-fair/api/v1/datasets/dpe03existant/lines"

# Nombre total de pages que l‚Äôon veut r√©cup√©rer
# (important pour ne pas surcharger l‚ÄôAPI)
TOTAL_PAGES = 20

# Nombre de lignes r√©cup√©r√©es par page
PAGE_SIZE = 1000

# ------------------------------------------------------------
# CONFIGURATION MINIO (stockage objet type S3)
# ------------------------------------------------------------

# Adresse du serveur MinIO dans le r√©seau Docker
# IMPORTANT : ici on utilise le nom du service ("minio") du docker-compose
MINIO_ENDPOINT = "http://minio:9000"

# Identifiants (d√©finis dans docker-compose.yml)
MINIO_ACCESS_KEY = "minioadmin"
MINIO_SECRET_KEY = "minioadmin"

# Nom du bucket (√©quivalent d‚Äôun "dossier racine" dans un Data Lake)
BUCKET_NAME = "ademe-data"

# ------------------------------------------------------------
# Cr√©ation du client S3 compatible MinIO avec Boto3
# ------------------------------------------------------------
s3_client = boto3.client(
    "s3",                            # type de service
    endpoint_url=MINIO_ENDPOINT,     # URL de MinIO
    aws_access_key_id=MINIO_ACCESS_KEY,
    aws_secret_access_key=MINIO_SECRET_KEY
)

# ------------------------------------------------------------
# Cr√©ation automatique du bucket s‚Äôil n‚Äôexiste pas encore
# ------------------------------------------------------------
buckets = [b["Name"] for b in s3_client.list_buckets()["Buckets"]]

if BUCKET_NAME not in buckets:
    print(f"Cr√©ation du bucket {BUCKET_NAME}")
    s3_client.create_bucket(Bucket=BUCKET_NAME)

# ------------------------------------------------------------
# FONCTIONS UTILES
# ------------------------------------------------------------

def fetch_page(page: int):
    """
    R√©cup√®re les donn√©es d'une page ADEME

    - On appelle l‚ÄôAPI
    - On filtre uniquement les champs utiles
    - On limite le p√©rim√®tre au d√©partement 69
    """
    params = {
        "size": PAGE_SIZE,
        "page": page,
        #"select": "numero_dpe,date_reception_dpe,code_postal_ban,etiquette_dpe",
        "qs": "code_departement_ban:69"
    }

    # Appel HTTP de l‚ÄôAPI
    r = requests.get(API_URL, params=params)

    # Gestion d'erreur simple : si l‚ÄôAPI ne renvoie pas du JSON
    try:
        return r.json().get("results", [])
    except ValueError:
        print(f"Erreur JSON pour la page {page}, contenu brut: {r.text[:200]}")
        return []

def save_to_minio(data, filename):
    """
    Sauvegarde des donn√©es dans MinIO (bucket S3)

    - Bucket = data lake
    - Key = chemin logique du fichier dans le bucket
    - Body = contenu JSON
    """
    s3_client.put_object(
        Bucket=BUCKET_NAME,
        Key=filename,                                      # ex: raw/page_1.json
        Body=json.dumps(data, ensure_ascii=False, indent=2),
        ContentType="application/json"
    )
    print(f"{filename} envoy√© dans le bucket {BUCKET_NAME}")

# ------------------------------------------------------------
# BOUCLE PRINCIPALE (Zone Bronze du Data Lake)
# ------------------------------------------------------------

# Cette boucle va construire la couche "Bronze" :
# ‚Üí donn√©es brutes, non transform√©es, stock√©es telles quelles
for page in range(1, TOTAL_PAGES + 1):
    print(f"R√©cup√©ration page {page}...")

    # R√©cup√©ration de la page API
    results = fetch_page(page)

    # Si des donn√©es existent, on les sauvegarde
    if results:
        # "raw/" correspond √† la couche Bronze du Data Lake
        filename = f"raw/page_{page}.json"
        save_to_minio(results, filename)
    else:
        print(f"Aucune donn√©e pour la page {page}")
```

### Script ‚Äì `json_to_csv.py`

Objectif : lire les JSON stock√©s dans MinIO et produire un **CSV consolid√©** (couche Silver).

```python
import json
import boto3
import pandas as pd
from io import BytesIO

# ------------------------------------------------------------
# Configuration MinIO
# ------------------------------------------------------------
MINIO_ENDPOINT = "http://minio:9000"
ACCESS_KEY = "minioadmin"
SECRET_KEY = "minioadmin"
BUCKET_NAME = "ademe-data"

# Connexion S3 vers MinIO
s3 = boto3.client(
    "s3",
    endpoint_url=MINIO_ENDPOINT,
    aws_access_key_id=ACCESS_KEY,
    aws_secret_access_key=SECRET_KEY
)

# Liste des objets dans la zone "raw/" (Bronze)
objects = s3.list_objects_v2(Bucket=BUCKET_NAME, Prefix="raw/")

# Liste Python qui va contenir toutes les lignes
all_rows = []

# ------------------------------------------------------------
# Lecture de tous les fichiers JSON
# ------------------------------------------------------------
for obj in objects.get("Contents", []):
    key = obj["Key"]

    # On ne lit que les fichiers JSON
    if key.endswith(".json"):
        print(f"Lecture {key}")

        # T√©l√©chargement du fichier depuis MinIO
        body = s3.get_object(Bucket=BUCKET_NAME, Key=key)["Body"].read()

        # Conversion JSON ‚Üí Python
        data = json.loads(body)

        # Ajout des lignes dans la liste globale
        all_rows.extend(data)

# ------------------------------------------------------------
# Transformation en DataFrame Pandas
# ------------------------------------------------------------
df = pd.DataFrame(all_rows)

# Sauvegarde du CSV en m√©moire (pas sur disque)
csv_buffer = BytesIO()
df.to_csv(csv_buffer, index=False)

# ------------------------------------------------------------
# Upload dans la couche "Silver"
# ------------------------------------------------------------
s3.put_object(
    Bucket=BUCKET_NAME,
    Key="processed/ademe_all.csv",     # Zone Silver
    Body=csv_buffer.getvalue(),
    ContentType="text/csv"
)

print("CSV cr√©√© et upload√© ‚úÖ")
```

### Script ‚Äì `csv_to_parquet.py`

Objectif : produire un **Parquet partitionn√©**, couche Gold du Data Lake.

```python
import json
import boto3
import pandas as pd
import pyarrow as pa
import pyarrow.parquet as pq
from io import BytesIO
import os

# ------------------------------------------------------------
# Configuration MinIO
# ------------------------------------------------------------
MINIO_ENDPOINT = "http://minio:9000"
ACCESS_KEY = "minioadmin"
SECRET_KEY = "minioadmin"
BUCKET_NAME = "ademe-data"

s3 = boto3.client(
    "s3",
    endpoint_url=MINIO_ENDPOINT,
    aws_access_key_id=ACCESS_KEY,
    aws_secret_access_key=SECRET_KEY
)

# ------------------------------------------------------------
# Lecture du fichier CSV (Silver)
# ------------------------------------------------------------

obj = s3.get_object(
    Bucket=BUCKET_NAME,
    Key="processed/ademe_all.csv"
)

df = pd.read_csv(BytesIO(obj["Body"].read()))

# Conversion Pandas ‚Üí Table PyArrow
table = pa.Table.from_pandas(df)

# ------------------------------------------------------------
# Ecriture locale du Parquet partitionn√©
# ------------------------------------------------------------
# Chaque valeur unique de "etiquette_dpe" aura son propre dossier
temp_folder = "/tmp/ademe_parquet/"

pq.write_to_dataset(
    table,
    root_path=temp_folder,
    partition_cols=["etiquette_dpe"]
)

# ------------------------------------------------------------
# Upload des fichiers Parquet vers MinIO (couche Gold)
# ------------------------------------------------------------
for root, dirs, files in os.walk(temp_folder):
    for file in files:
        local_path = os.path.join(root, file)

        # Chemin relatif dans MinIO
        relative_path = os.path.relpath(local_path, temp_folder)

        # Cl√© finale S3
        s3_key = f"final/parquet/{relative_path}"

        # Lecture du fichier local et upload dans MinIO
        with open(local_path, "rb") as f:
            s3.put_object(Bucket=BUCKET_NAME, Key=s3_key, Body=f.read())

        print(f"Upload {s3_key} ‚úÖ")

print("Parquet partitionn√© cr√©√© et upload√© ‚úÖ")
```

**Explication :**
`PyArrow.write_to_dataset()` ne supporte pas directement l‚Äôupload S3
La fonction `pq.write_to_dataset`est con√ßue pour √©crire sur un filesystem local ou compatible (type HDFS, S3 via fsspec, etc.).
Si tu voulais √©crire directement sur MinIO, il faudrait configurer un filesystem S3 compatible avec fsspec ou Spark. Cela complique le script et n√©cessite souvent des d√©pendances suppl√©mentaires.

### Commandes docker

Avant de lancer les scripts Python, il faut construire et d√©marrer l‚Äôinfrastructure MinIO + Python.

1. Construction des images Docker

```bash
cd ./tp-minio-ademe
docker compose build
```

**Explication :**
Cette commande :
- lit le `Dockerfile`
- installe Python et les d√©pendances (`boto3`, `requests`, `pandas`, `pyarrow`)
- pr√©pare l‚Äôimage du conteneur Python

2. D√©marrage des services

```bash
docker compose up -d
```

**Explication :**
- `up` : d√©marre les conteneurs
- `-d` (detached) : lance les conteneurs en arri√®re-plan

√Ä ce stade :
- MinIO est en cours d‚Äôex√©cution
- le conteneur Python est pr√™t

3. Acc√®s √† l‚Äôinterface MinIO

Ouvrir le navigateur :

```
http://localhost:9001
```

Puis se connecter avec :

- **Login** : `minioadmin`  
- **Password** : `minioadmin`

:warning: √Ä ce moment-l√†, **aucun bucket n‚Äôexiste encore**, car il sera cr√©√© automatiquement par le script Python.

4. Ingestion des donn√©es (Couche Bronze)

```bash
docker compose run python python fetch_ademe_to_minio.py
```

Ce script va :
- appeler l‚ÄôAPI ADEME
- cr√©er le bucket `ademe-data` si besoin
- stocker les fichiers JSON dans le dossier logique :

```
raw/
```

5. Transformation en CSV (Couche Silver)

```bash
docker compose run python python json_to_csv.py
```

Ce script va :
- lire tous les fichiers JSON depuis MinIO
- les regrouper dans un seul DataFrame
- cr√©er un fichier CSV

Stock√© dans :

```
processed/ademe_all.csv
```

6. Conversion en Parquet (Couche Gold)

```bash
docker compose run python python csv_to_parquet.py
```

Ce script va :
- convertir les donn√©es en format Parquet
- appliquer un **partitionnement** par `etiquette_dpe`
- uploader les fichiers dans :

```
final/parquet/
```

### R√©sultat  attendu

Dans MinIO, nous verrons :

```
ademe-data/
 ‚îú‚îÄ‚îÄ raw/            (JSON bruts ‚Üí Bronze)
 ‚îú‚îÄ‚îÄ processed/      (CSV ‚Üí Silver)
 ‚îî‚îÄ‚îÄ final/
     ‚îî‚îÄ‚îÄ parquet/    (Parquet partitionn√© ‚Üí Gold)
```

### Tester la persistance des donn√©es MinIO avec les volumes Docker

1. Arr√™te et supprime uniquement les **conteneurs** :

```bash
docker compose down
```

√Ä ce stade :
- les conteneurs sont supprim√©s
- le volume `minio_data` est toujours pr√©sent
Red√©marrer l‚Äôenvironnement

2. Relance les services :

```bash
docker compose up -d
```

3. Puis retourne sur :

```
http://localhost:9001
```

**Conclusion :**  
Les donn√©es ne sont pas stock√©es dans le conteneur mais dans le **volume Docker**.


## TD : Gestion des utilisateurs et policies

Dans ce TP, nous allons approfondir l‚Äôutilisation de MinIO en cr√©ant des utilisateurs, en leur assignant des policies (droits d‚Äôacc√®s) et en testant la connexion depuis Python.  
Nous r√©utilisons le TP1 pour les scripts de gestion des donn√©es ADEME, mais nous ajoutons une couche de s√©curit√© et de contr√¥le d‚Äôacc√®s.

### Arborescence du projet

```
tp-minio-ademe/
‚îú‚îÄ‚îÄ docker-compose.yml
‚îú‚îÄ‚îÄ python/
‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile
‚îÇ   ‚îú‚îÄ‚îÄ requirements.txt
‚îÇ   ‚îú‚îÄ‚îÄ test_users.py
‚îÇ   ‚îú‚îÄ‚îÄ fetch_ademe_to_minio.py
‚îÇ   ‚îú‚îÄ‚îÄ json_to_csv.py
‚îÇ   ‚îî‚îÄ‚îÄ csv_to_parquet.py
‚îú‚îÄ‚îÄ ubuntu/
‚îÇ   ‚îî‚îÄ‚îÄ Dockerfile
‚îú‚îÄ‚îÄ policies/
‚îÇ   ‚îú‚îÄ‚îÄ read-policy.json
‚îÇ   ‚îî‚îÄ‚îÄ write-policy.json
```

Explications :

- `python/` : contient tous les scripts Python du TP1 et le nouveau script `test_users.py`.  
- `ubuntu/` : contiendra un Dockerfile pour un conteneur Ubuntu temporaire utilis√© pour configurer `mc`, cr√©er les utilisateurs et les policies.  
- `policies/` : fichiers JSON d√©crivant les droits en lecture et √©criture pour MinIO.

MinIO fournit un client officiel appel√© **`mc` (MinIO Client)** qui permet de‚ÄØ:  
- Cr√©er des alias pour se connecter √† un serveur MinIO.  
- Ajouter et g√©rer des utilisateurs.  
- Cr√©er et attacher des policies (droits en lecture/√©criture) √† ces utilisateurs.  

:warning: Le serveur MinIO ne contient que le serveur et la console web, il ne permet pas de cr√©er des utilisateurs ni des policies.  Le conteneur Ubuntu sert donc de **station d‚Äôadministration portable**, compatible sur tous les syst√®mes et r√©utilisable.

### ubuntu/Dockerfile

```dockerfile
FROM ubuntu:22.04

# Installer wget pour t√©l√©charger mc (MinIO client)
RUN apt-get update && apt-get install -y wget curl && \
    rm -rf /var/lib/apt/lists/*

WORKDIR /mc

# T√©l√©charger et rendre ex√©cutable le client mc
RUN wget https://dl.min.io/client/mc/release/linux-amd64/mc && chmod +x mc

ENV PATH="/mc:${PATH}"
```

Ce conteneur Ubuntu est utilis√© pour ex√©cuter le client `mc` et configurer MinIO : alias, cr√©ation d‚Äôutilisateurs et attribution des policies.


### policies/read-policy.json

```json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Action": [
        "s3:GetBucketLocation",
        "s3:ListBucket",
        "s3:GetObject"
      ],
      "Effect": "Allow",
      "Resource": ["arn:aws:s3:::*"]
    }
  ]
}
```

### policies/write-policy.json

```json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Action": [
        "s3:GetBucketLocation",
        "s3:ListBucket",
        "s3:PutObject",
        "s3:DeleteObject"
      ],
      "Effect": "Allow",
      "Resource": ["arn:aws:s3:::*"]
    }
  ]
}
```

Explication :
- `read-policy` : acc√®s en lecture seule (liste des buckets, lecture des objets).  
- `write-policy` : lecture/√©criture et suppression des objets.  


### python/test_users.py

```python
import boto3

MINIO_ENDPOINT = "http://minio:9000"

users = {
    "etl-user": "etlpassword",
    "reader-user": "readerpassword"
}

# Test de connexion pour chaque utilisateur et r√©cup√©ration des buckets
for user, pwd in users.items():
    print(f"Test connexion pour {user}...")
    try:
        s3 = boto3.client(
            "s3",
            endpoint_url=MINIO_ENDPOINT,
            aws_access_key_id=user,
            aws_secret_access_key=pwd
        )
        buckets = s3.list_buckets()
        print(f"‚úÖ {user} connect√©. Buckets: {[b['Name'] for b in buckets['Buckets']]}")
    except Exception as e:
        print(f"‚ùå Erreur pour {user}: {e}")
```

Explication :
Ce script permet aux √©tudiants de v√©rifier que les utilisateurs et leurs droits fonctionnent correctement depuis Python, gr√¢ce √† la compatibilit√© S3 de MinIO.


### Commandes √† ex√©cuter


1. Build du conteneur Ubuntu pour mc

```bash
docker build -t tp2-ubuntu ./ubuntu
```

2. V√©rifier le r√©seau Docker du TP2

```bash
docker network ls
```

Rep√©rer le r√©seau par d√©faut cr√©√© par `docker-compose`. Par exemple : `tp-minio-ademe_default`.

3. Lancer un conteneur Ubuntu temporaire pour cr√©er les users et policies

```bash
docker run -it --network tp-minio-ademe_default -v ${PWD}/policies:/policies tp2-ubuntu
```

Puis, √† l‚Äôint√©rieur du conteneur Ubuntu :

```bash
mc alias set local http://minio:9000 minioadmin minioadmin

mc admin user add local etl-user etlpassword
mc admin user add local reader-user readerpassword

mc admin policy create local write-policy /policies/write-policy.json
mc admin policy create local read-policy /policies/read-policy.json

mc admin policy attach local write-policy --user etl-user
mc admin policy attach local read-policy --user reader-user
```

Ensuite quitter le conteneur avec `exit`.

4. Lancer le script Python de test des utilisateurs

:bulb: Pas besoin de rebuild le conteneur Python si aucun package ou Dockerfile n‚Äôa √©t√© modifi√©, car le volume mont√© contient d√©j√† tous les scripts.  

```bash
docker compose run python python test_users.py
```

:bulb: V√©rifie que chaque utilisateur peut se connecter et que ses droits (lecture/√©criture) sont appliqu√©s correctement.


5. Tester les policies directement dans l'interface MinIO.


6. Modifier les scripts python du TP1 en utilisant les acc√®s du user `etl-user`



## TD : MinIO et Apache Iceberg

Dans ce TP, vous allez :

- Comprendre comment **Spark interagit avec un Data Lake**
- Lire des donn√©es depuis MinIO via le protocole **S3A** (S3A est le connecteur Hadoop/Spark qui permet d‚Äôacc√©der √† MinIO ou Amazon S3 comme un syst√®me de fichiers.)
- Transformer des fichiers **Parquet** dans une **table Iceberg**

Ce TP vous permet de simuler une architecture **Lakehouse** comme en entreprise.

### Arborescence du projet

```bash
tp-minio-ademe/
‚îú‚îÄ‚îÄ docker-compose.yml
‚îú‚îÄ‚îÄ python/
‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile
‚îÇ   ‚îú‚îÄ‚îÄ requirements.txt
‚îÇ   ‚îú‚îÄ‚îÄ test_users.py
‚îÇ   ‚îú‚îÄ‚îÄ fetch_ademe_to_minio.py
‚îÇ   ‚îú‚îÄ‚îÄ json_to_csv.py
‚îÇ   ‚îî‚îÄ‚îÄ csv_to_parquet.py
‚îú‚îÄ‚îÄ ubuntu/
‚îÇ   ‚îî‚îÄ‚îÄ Dockerfile
‚îú‚îÄ‚îÄ policies/
‚îÇ   ‚îú‚îÄ‚îÄ read-policy.json
‚îÇ   ‚îî‚îÄ‚îÄ write-policy.json
‚îî‚îÄ‚îÄ spark/
    ‚îú‚îÄ‚îÄ Dockerfile
    ‚îú‚îÄ‚îÄ spark-defaults.conf
    ‚îî‚îÄ‚îÄ jobs/
        ‚îî‚îÄ‚îÄ job_parquet_to_iceberg.py
```


### Configuration Spark

1. `spark/spark-defaults.conf`

Ce fichier configure Spark pour :

- Charger les **JARs n√©cessaires**
- Connecter Spark √† MinIO via **S3A**
- Activer le **catalogue Iceberg**

```properties
spark.jars=/opt/jars/iceberg-spark-runtime-3.5_2.12-1.6.0.jar,\
/opt/jars/hadoop-aws-3.3.4.jar,\
/opt/jars/aws-java-sdk-bundle-1.12.700.jar

# D√©claration du catalogue Iceberg
spark.sql.catalog.iceberg=org.apache.iceberg.spark.SparkCatalog
spark.sql.catalog.iceberg.type=hadoop
spark.sql.catalog.iceberg.warehouse=s3a://ademe-data/

# Connexion √† MinIO
spark.hadoop.fs.s3a.endpoint=http://minio:9000
spark.hadoop.fs.s3a.access.key=minioadmin
spark.hadoop.fs.s3a.secret.key=minioadmin
spark.hadoop.fs.s3a.path.style.access=true

# Activation Iceberg
spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions
```

üí° **Pourquoi c‚Äôest important ?**  
Sans ces param√®tres, Spark ne peut pas lire/√©crire dans MinIO ni utiliser Iceberg.

2. `spark/Dockerfile`

Ce Dockerfile :

- Utilise une image Spark officielle
- T√©l√©charge dynamiquement les d√©pendances
- Ajoute la configuration Spark

```dockerfile
FROM apache/spark:3.5.0

USER root
RUN mkdir -p /opt/jars

# D√©pendances n√©cessaires
ADD https://repo1.maven.org/maven2/org/apache/iceberg/iceberg-spark-runtime-3.5_2.12/1.6.0/iceberg-spark-runtime-3.5_2.12-1.6.0.jar /opt/jars/
ADD https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/3.3.4/hadoop-aws-3.3.4.jar /opt/jars/
ADD https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/1.12.700/aws-java-sdk-bundle-1.12.700.jar /opt/jars/

# Configuration Spark
COPY spark-defaults.conf /opt/spark/conf/spark-defaults.conf

# Jobs Spark
COPY jobs /jobs
```

**Pourquoi ces JARs ?**  
Ils permettent √† Spark de :
- Parler le protocole S3 (`hadoop-aws`)
- Comprendre Iceberg

3. `spark/jobs/job_parquet_to_iceberg.py`

```python
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("ParquetToIceberg").getOrCreate()

print("Lecture des fichiers Parquet depuis MinIO...")
df = spark.read.parquet("s3a://ademe-data/final/parquet/")

print("Aper√ßu des donn√©es")
df.show(5)
df.printSchema()

print("√âcriture des donn√©es au format Iceberg...")
df.writeTo("iceberg.gold.dpe_table").createOrReplace()

print("Job termin√© ‚úÖ")
```

**Explication** :
- `spark.read.parquet()` lit directement les fichiers depuis MinIO
- `writeTo()` cr√©e une table Iceberg logique
- Il n‚Äôy a **aucun stockage local**, tout passe par S3A

4. Service Spark dans docker-compose

√Ä ajouter dans ton `docker-compose.yml` :

```yaml
  spark:
    build: ./spark
    container_name: spark
    depends_on:
      - minio
    volumes:
      - ./spark/jobs:/jobs
```

**Explication** :
- `build: ./spark` ‚Üí construit l‚Äôimage Docker depuis ton dossier `spark/`
- `depends_on` ‚Üí d√©marre MinIO avant Spark
- `volumes` ‚Üí permet de modifier les jobs sans reconstruire l‚Äôimage

---

### Commandes Docker expliqu√©es

1. Construire les images

```bash
docker compose build
```

Cette commande :
- Lit les `Dockerfile`
- T√©l√©charge Spark + d√©pendances
- Pr√©pare toutes les images


2. D√©marrer les services

```bash
docker compose up -d
```

D√©marre :
- MinIO
- Spark
En arri√®re-plan (`-d` = detached).

3. Lister les conteneurs actifs

```bash
docker ps
```

V√©rifie que :
- `minio`
- `spark`
fonctionnent correctement.

4. Lancer le job Spark

```bash
docker compose run spark /opt/spark/bin/spark-submit /jobs/job_parquet_to_iceberg.py
```

Cette commande :
- Lance un conteneur Spark temporaire
- Ex√©cute ton job Python
- Se connecte √† MinIO
- Lit Parquet ‚Üí √âcrit Iceberg

5. R√©sultat attendu

Si tout fonctionne :

- Spark affiche un aper√ßu des donn√©es
- La table Iceberg est cr√©√©e
- Les m√©tadonn√©es apparaissent dans MinIO
- Le job se termine par :  
  `Job termin√© ‚úÖ`
